{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics for the manuscript Table 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"../../output/mimic_cxr/coref_voting/temp_for_silver/ensemble_1k\"\n",
    "csv_col_name = \"[mv]coref_group_conll\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "sys.path.append(\"../../../../git_clone_repos/fast-coref/src\")\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, HTML\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import Event\n",
    "from common_utils.file_checker import FileChecker\n",
    "from common_utils.common_utils import check_and_create_dirs, check_and_remove_dirs\n",
    "import ast\n",
    "from collections import Counter\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "FILE_CHECKER = FileChecker()\n",
    "START_EVENT = Event()\n",
    "\n",
    "SEED_NUM = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_mention_and_group_num(df: pd.DataFrame, conll_colName: str, omit_singleton=True) -> tuple[int, int]:\n",
    "    \"\"\"Args:\n",
    "        df: The dataframe resolved from csv file.\n",
    "        conll_colName: The name of the column with conll format elements.\n",
    "        omit_singleton: Omit singleton mention and the corresponding coref group.\n",
    "\n",
    "    Return:\n",
    "        The number of coreference mentions and coreference groups.\n",
    "    \"\"\"\n",
    "    corefGroup_counter = Counter()\n",
    "    # Only the cells that contain str will be included\n",
    "    conll_corefGroup_str_list = df[~df.loc[:, conll_colName].isin([\"-1\", -1.0, np.nan])].loc[:, conll_colName].to_list()\n",
    "    # The index of those cells.\n",
    "    conll_corefGroup_idx_list = df[~df.loc[:, conll_colName].isin([\"-1\", -1.0, np.nan])].index.tolist()\n",
    "    cluster_id_stack = [] # the cluster id of a mention\n",
    "    mention_idx_stack = [] # the index of a mention\n",
    "    \n",
    "    cluster_tokenNums_dict = defaultdict(list)\n",
    "    for cell_idx, conll_corefGroup_cell_str in zip(conll_corefGroup_idx_list, conll_corefGroup_str_list):\n",
    "        if isinstance(conll_corefGroup_cell_str, list):\n",
    "            conll_corefGroup_str_list = conll_corefGroup_cell_str\n",
    "        else:\n",
    "            conll_corefGroup_str_list = ast.literal_eval(conll_corefGroup_cell_str)\n",
    "        for conll_corefGroup_str in conll_corefGroup_str_list:\n",
    "            str_start = re.search(r\"\\((\\d+)\", conll_corefGroup_str)\n",
    "            str_end = re.search(r\"(\\d+)\\)\", conll_corefGroup_str)\n",
    "            if str_start:\n",
    "                cluster_id_stack.insert(0,int(str_start.group(1)))\n",
    "                mention_idx_stack.insert(0, cell_idx)\n",
    "            if str_end:\n",
    "                idx = cluster_id_stack.index(int(str_end.group(1)))\n",
    "                del cluster_id_stack[idx]\n",
    "                start_idx = mention_idx_stack.pop(idx)\n",
    "                mention_length = cell_idx - start_idx + 1\n",
    "                cluster_id = int(str_end.group(1))\n",
    "                cluster_tokenNums_dict[cluster_id].append(mention_length)\n",
    "                corefGroup_counter.update([cluster_id])\n",
    "    if omit_singleton:\n",
    "        non_singletone_counter: list[tuple] = list(filter(lambda item: item[1] > 1, corefGroup_counter.items()))\n",
    "        cluster_mention_num_list = [v for k, v in non_singletone_counter]\n",
    "        coref_mention_num = sum(cluster_mention_num_list)\n",
    "        if cluster_mention_num_list:\n",
    "            max_cluster_mention_num = max(cluster_mention_num_list)\n",
    "        else:\n",
    "            max_cluster_mention_num = 0\n",
    "        coref_group_num = len([k for k, v in non_singletone_counter])\n",
    "        token_num_list = []\n",
    "        for cluster_id, _ in non_singletone_counter:\n",
    "            token_num_list.extend(cluster_tokenNums_dict[cluster_id])\n",
    "        mention_tok_num = sum(token_num_list)\n",
    "        \n",
    "    return coref_mention_num, coref_group_num, mention_tok_num, max_cluster_mention_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_processing(src_dir, section_name, sid, spacy_input_path):\n",
    "    START_EVENT.wait()\n",
    "    df = pd.read_csv(os.path.join(src_dir,section_name,sid+\".csv\"), index_col=0, na_filter=False)\n",
    "\n",
    "    token_list = df.loc[:,\"[sp]token\"].to_list()\n",
    "    token_num = len(token_list)\n",
    "    mention_num, group_num, mention_tok_num, max_cluster_mention_num = resolve_mention_and_group_num(df, csv_col_name)\n",
    "\n",
    "    return sid, token_num, mention_num, group_num, mention_tok_num, max_cluster_mention_num\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing section: impression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "846it [00:00, 15413.47it/s]\n",
      "100%|██████████| 846/846 [00:01<00:00, 667.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing section: findings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "846it [00:00, 11065.59it/s]\n",
      "100%|██████████| 846/846 [00:01<00:00, 651.45it/s]\n"
     ]
    }
   ],
   "source": [
    "src_dir = csv_file_path\n",
    "\n",
    "doc_list = []\n",
    "token_num_list = []\n",
    "mention_num_list = []\n",
    "cluster_num_list = []\n",
    "mention_tok_num_list = []\n",
    "max_cluster_mention_num_list = []\n",
    "\n",
    "for section_entry in os.scandir(src_dir):\n",
    "    if section_entry.is_dir():\n",
    "        print(\"Processing section:\", section_entry.name)\n",
    "\n",
    "        tasks = []\n",
    "        scatter_data_list:list[dict] = []\n",
    "        with ProcessPoolExecutor(max_workers=8) as executor:\n",
    "            for report_entry in tqdm(os.scandir(section_entry.path)):\n",
    "                if FILE_CHECKER.ignore(os.path.abspath(report_entry.path)):\n",
    "                    continue\n",
    "                sid = report_entry.name.rstrip(\".csv\")\n",
    "                tasks.append(executor.submit(batch_processing, src_dir, section_entry.name, sid, report_entry.path))\n",
    "\n",
    "            START_EVENT.set()\n",
    "\n",
    "            # Receive results from multiprocessing.\n",
    "            for future in tqdm(as_completed(tasks), total=len(tasks)):\n",
    "                sid, token_num, mention_num, group_num, mention_tok_num, max_cluster_mention_num = future.result()\n",
    "                doc_list.append(sid)\n",
    "                token_num_list.append(token_num)\n",
    "                mention_num_list.append(mention_num)\n",
    "                cluster_num_list.append(group_num)\n",
    "                mention_tok_num_list.append(mention_tok_num)\n",
    "                max_cluster_mention_num_list.append(max_cluster_mention_num)\n",
    "            START_EVENT.clear()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "document num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1692"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens / Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117.46926713947991"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(token_num_list) / len(doc_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mentions / Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.186761229314421"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mention_num_list) / len(doc_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters / Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4781323877068557"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cluster_num_list) / len(doc_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens / Mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.841246290801187"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mention_tok_num_list) / sum(mention_num_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mentions / Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.15593762495002"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mention_num_list) / sum(cluster_num_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Number of Mentions in Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(max_cluster_mention_num_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sr_coref",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
