{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create unlabeled pool\n",
    "\n",
    "Our MIMIC-CXR dataset (splited by sections) has too many docs that have no coreference clusters. \n",
    "\n",
    "We need to create a subset for the unlabeled pool in which docs are more likely having coreference clusters.\n",
    "\n",
    "The preliminary statistic is done by the original fast-coref model.\n",
    "\n",
    "The subset need to remove the gt_test files.\n",
    "\n",
    "The subset is sampling from the candidate dataset used for generating other silver/manual/semi-manual datasets. For the efficiency, we restrict the size of the subset to 5k docs.\n",
    "\n",
    "The `unlabeled pool` will be generated at `output/mimic_cxr/active_learning/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../src\")\n",
    "sys.path.append(\"../../../../git_clone_repos/fast-coref/src\")\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, HTML\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import Event\n",
    "from common_utils.data_loader_utils import load_mimic_cxr_bySection\n",
    "from common_utils.coref_utils import (\n",
    "    resolve_mention_and_group_num,\n",
    "    shuffle_list,\n",
    "    ConllToken,\n",
    "    check_and_make_dir,\n",
    "    get_data_split,\n",
    "    get_file_name_prefix,\n",
    "    get_porportion_and_name,\n",
    "    remove_all,\n",
    "    resolve_mention_and_group_num,\n",
    "    shuffle_list,\n",
    ")\n",
    "from common_utils.file_checker import FileChecker\n",
    "from common_utils.common_utils import check_and_create_dirs, check_and_remove_dirs\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "FILE_CHECKER = FileChecker()\n",
    "START_EVENT = Event()\n",
    "\n",
    "SEED_NUM = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "config = None\n",
    "with initialize(version_base=None, config_path=\"../config\", job_name=\"nlp_ensemble\"):\n",
    "    config = compose(config_name=\"data_preprocessing\", overrides=[\"+nlp_ensemble@_global_=mimic_cxr\"])\n",
    "\n",
    "section_name_cfg = config.name_style.mimic_cxr.section_name\n",
    "output_section_cfg = config.output.section\n",
    "input_path = config.input.path\n",
    "data_size, pid_list, sid_list, section_list = load_mimic_cxr_bySection(input_path, output_section_cfg, section_name_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort\n",
    "s_list, f_list, i_list, pfi_list, fai_list = zip(\n",
    "    *sorted(zip(sid_list, section_list[0][1], section_list[1][1], section_list[2][1], section_list[3][1]))\n",
    ")\n",
    "sid_list = s_list\n",
    "section_list = [\n",
    "    (\"findings\", f_list),\n",
    "    (\"impression\", i_list),\n",
    "    (\"provisional_findings_impression\", pfi_list),\n",
    "    (\"findings_and_impression\", fai_list),\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fcoref_dir` is the folder that our unlabeled pool data come from. It contains section reports in csv format in which the contant that are tokenized by spacy with coref results predicted by fast-coref model.\n",
    "\n",
    "`section_corefNum_docs_dict_file` is the statistical result regarding the number of coref cluster for each document in the `fcoref_dir`. This file is the side-product from `generate_silver_trainset.ipynb`-`#Prepare`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_corefNum_docs_dict_file = \"../../output/mimic_cxr/nlp_ensemble/fast_coref_joint_(stripped_input).statistic\"\n",
    "fcoref_dir = \"../../output/mimic_cxr/nlp_ensemble/fast_coref_joint_(stripped_input)\"\n",
    "\n",
    "section_corefNum_docs_dict = {}\n",
    "with open(section_corefNum_docs_dict_file, \"r\") as f:\n",
    "    a = f.readlines()\n",
    "temp = json.loads(\"\".join(a))\n",
    "for section_name, corefNum_docs_dict in temp.items():\n",
    "    section_corefNum_docs_dict[section_name] = {}\n",
    "    for corefNumStr, docList in corefNum_docs_dict.items():\n",
    "        section_corefNum_docs_dict[section_name][int(corefNumStr)] = docList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"findings\": {\"0\": 135615, \"1\": 16800, \"2\": 2804, \"3\": 590, \"4\": 154, \"5\": 27, \"6\": 13, \"7\": 6, \"8\": 2}, \"impression\": {\"0\": 173650, \"1\": 13016, \"2\": 2155, \"3\": 506, \"4\": 100, \"5\": 28, \"6\": 8, \"7\": 1}}\n"
     ]
    }
   ],
   "source": [
    "section_corefNum_docNum_dict = {}\n",
    "for section_name in [\"findings\", \"impression\"]:\n",
    "    corefNum_docs_dict = section_corefNum_docs_dict[section_name]\n",
    "    section_corefNum_docNum_dict[section_name] = {}\n",
    "    corefNum = 0\n",
    "    while True:\n",
    "        if corefNum not in corefNum_docs_dict:\n",
    "            break\n",
    "        section_corefNum_docNum_dict[section_name][corefNum] = len(corefNum_docs_dict[corefNum])\n",
    "        corefNum += 1\n",
    "print(json.dumps(section_corefNum_docNum_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "2500\n",
      "{'findings': {1: 854, 2: 854, 3: 590, 4: 154, 5: 27, 6: 13, 7: 6, 8: 2}, 'impression': {1: 929, 2: 928, 3: 506, 4: 100, 5: 28, 6: 8, 7: 1}}\n"
     ]
    }
   ],
   "source": [
    "expected_sampling = {\"findings\": 2500, \"impression\": 2500}\n",
    "\n",
    "target_sampling = {\n",
    "    \"findings\": {1: None, 2: None, 3: 590, 4: 154, 5: 27, 6: 13, 7: 6, 8: 2},  # 846 - the rest\n",
    "    \"impression\": {1: None, 2: None, 3: 506, 4: 100, 5: 28, 6: 8, 7: 1},  # 846 - the rest\n",
    "}\n",
    "\n",
    "# Assign the rest number in average for the None value item.\n",
    "for section_name in [\"findings\", \"impression\"]:\n",
    "    empty_ids = [k for k,v in target_sampling[section_name].items() if not v]\n",
    "    rest_avg_num = (expected_sampling[section_name] - sum([i for i in target_sampling[section_name].values() if i])) // len(empty_ids)\n",
    "    for key_id in empty_ids:\n",
    "        target_sampling[section_name][key_id] = rest_avg_num\n",
    "    \n",
    "    over_num = sum([i for i in target_sampling[section_name].values() if i]) - expected_sampling[section_name]\n",
    "    target_sampling[section_name][1] -= over_num\n",
    "    print(sum([i for i in target_sampling[section_name].values() if i]))\n",
    "\n",
    "print(target_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "from nlp_ensemble.nlp_menbers import play_fastcoref\n",
    "\n",
    "config = None\n",
    "with initialize(version_base=None, config_path=\"../config\", job_name=\"coreference_resolution\"):\n",
    "    config = compose(\n",
    "        config_name=\"coreference_resolution\",\n",
    "        overrides=[\"+coreference_resolution/data_preprocessing@_global_=mimic_cxr\"],\n",
    "    )\n",
    "shuffle_seed = config.shuffle_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose docs for the unlabeled pool, excluded docs that used in gt\n",
    "docId_testset_list = [\n",
    "    i.rstrip(\".csv\")\n",
    "    for i in FILE_CHECKER.filter(\n",
    "        os.listdir(os.path.join(\"../../output/mimic_cxr/manual_test_set/round1x2\", section_name))\n",
    "    )\n",
    "]\n",
    "unlabeled_pool_docs_list = {}\n",
    "for section_name in [\"findings\", \"impression\"]:\n",
    "    unlabeled_pool_docs_list[section_name] = defaultdict(list)\n",
    "    section_all_docNum = 0\n",
    "    for groupNum in sorted(target_sampling[section_name], reverse=True):\n",
    "        docNum = target_sampling[section_name][groupNum]\n",
    "        candidate_docId_list = section_corefNum_docs_dict[section_name][groupNum]\n",
    "        candidate_docId_list_exclude = [x for x in candidate_docId_list if x not in docId_testset_list]\n",
    "        candidate_docId_list_shuffle = shuffle_list(candidate_docId_list_exclude, shuffle_seed)\n",
    "        unlabeled_pool_docs_list[section_name][groupNum] = candidate_docId_list_shuffle[0:docNum]\n",
    "        section_all_docNum += len(unlabeled_pool_docs_list[section_name][groupNum])\n",
    "    \n",
    "    # After removing test files from the unlabeled pool, \n",
    "    # we need a few extra documents to meet the expected sampling number.\n",
    "    while section_all_docNum < expected_sampling[section_name]:\n",
    "        candidate_docId_list = section_corefNum_docs_dict[section_name][1]\n",
    "        candidate_docId_list_exclude_gt = [x for x in candidate_docId_list if x not in docId_testset_list]\n",
    "        candidate_docId_list_exclude_inUse = [x for x in candidate_docId_list if x not in unlabeled_pool_docs_list[section_name][1]]\n",
    "        doc = candidate_docId_list_exclude_inUse.pop(-1)\n",
    "        if doc not in unlabeled_pool_docs_list[section_name][1]:\n",
    "            unlabeled_pool_docs_list[section_name][1].append(doc)\n",
    "            section_all_docNum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "for section_name, corefGroupNum_docId_dict in unlabeled_pool_docs_list.items():\n",
    "    section_num = 0\n",
    "    for group_num, doc_list in corefGroupNum_docId_dict.items():\n",
    "        section_num += len(doc_list)\n",
    "    print(section_num)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`unlabeled_pool_dir` is the folder that the unlabeled pool data to be placed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "source_csv_dir = \"../../output/mimic_cxr/nlp_ensemble/spacy\"\n",
    "unlabeled_pool_dir = \"../../output/mimic_cxr/active_learning/unlabeled_pool_5k\"\n",
    "\n",
    "for section_name, data_dict in unlabeled_pool_docs_list.items():\n",
    "    csv_source_dir = os.path.join(source_csv_dir, section_name)\n",
    "    csv_target_dir = os.path.join(unlabeled_pool_dir, section_name)\n",
    "    check_and_create_dirs(csv_target_dir)\n",
    "    for group_num, doc_list in data_dict.items():\n",
    "        for file_name in doc_list:\n",
    "            csv_source_file = os.path.join(csv_source_dir, file_name+\".csv\")\n",
    "            csv_dst_file = os.path.join(csv_target_dir, file_name+\".csv\")\n",
    "            shutil.copy(csv_source_file, csv_dst_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sr_coref",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
