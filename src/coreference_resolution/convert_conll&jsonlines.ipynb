{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# From csv to conll / jsonlines\n",
                "\n",
                "For the eval.ipynb, we need to get individual conll files for evaulation\n",
                "1. Run `Prepare` and `Step 1`\n",
                "\n",
                "----\n",
                "\n",
                "For training fast-coref model, we need the generate aggregrated .conll and .jsonlines files\n",
                "\n",
                "The manually labeled training data folders are:\n",
                "- round4_100_1\n",
                "- round4_200_1x2\n",
                "- round4_300_123\n",
                "- round4_400_1234\n",
                "- round4_500_1234r3\n",
                "\n",
                "Remove docs that have 0 coref:\n",
                "1. Set `base_output_dir_name=gold_no0coref_all`, set `keep_0_coref_docs=False`; \n",
                "2. Set `source_csv_dir_name` to be the above 5 folder names\n",
                "3. Run `Prepare` and `Step 1 & 2 & 3`\n",
                "\n",
                "Keep docs that have 0 coref:\n",
                "1. Set `base_output_dir_name=gold_keep0coref_all`, set `keep_0_coref_docs=True`; \n",
                "2. Set `source_csv_dir_name` to be the above 5 folder names and repeat the script 5 times\n",
                "3. Run `Prepare` and `Step 1 & 2 & 3`\n",
                "\n",
                "Make `keep0coref` and `no0coref` datasets identical (organize the data in the same order):\n",
                "1. Run `Step 4`\n",
                "\n",
                "Create `unsplit` dataset for the experiment regarding number of validation:\n",
                "1. Run `Step 5`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "base_output_dir_name = \"gold_keep0coref_new_all\"\n",
                "# If False, then the 0 coref docs will not be included in the train/dev set\n",
                "keep_0_coref_docs = True"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prepare"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append(\"../../src\")\n",
                "sys.path.append(\"../../../../git_clone_repos/fast-coref/src/\")\n",
                "\n",
                "import os\n",
                "from tqdm import tqdm\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib as mpl\n",
                "import json\n",
                "import logging\n",
                "\n",
                "from IPython.display import display, HTML\n",
                "# display(HTML(df.to_html()))\n",
                "\n",
                "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
                "from multiprocessing import Event\n",
                "from common_utils.data_loader_utils import load_mimic_cxr_bySection\n",
                "from common_utils.coref_utils import resolve_mention_and_group_num\n",
                "from common_utils.file_checker import FileChecker\n",
                "from common_utils.common_utils import check_and_create_dirs, check_and_remove_dirs\n",
                "from data_preprocessing import mimic_cxr_csv2conll, mimic_cxr_conll2jsonlines\n",
                "\n",
                "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
                "\n",
                "FILE_CHECKER = FileChecker()\n",
                "START_EVENT = Event()\n",
                "logger = logging.getLogger()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from hydra import compose, initialize\n",
                "from omegaconf import OmegaConf\n",
                "\n",
                "config = None\n",
                "with initialize(version_base=None, config_path=\"../config\", job_name=\"coreference_resolution\"):\n",
                "        config = compose(config_name=\"coreference_resolution\", overrides=[\"+coreference_resolution/data_preprocessing@_global_=mimic_cxr\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Generate individual conll files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "source_csv_dir_name = \"round4_500_1234r3_new\"\n",
                "# Source input dir (csv)\n",
                "config.input_gt.base_dir = \"../../output/mimic_cxr/manual_training_set/\" + source_csv_dir_name\n",
                "# Target output dir (conll)\n",
                "config.temp_gt.base_dir = \"../../output/mimic_cxr/coref/individual_conll_ground_truth/\" + source_csv_dir_name\n",
                "config.temp_gt.force_run = False # Force to delete and recreate\n",
                "\n",
                "# source_csv_dir_name = \"round1x2_new\"\n",
                "# # Source input dir (csv)\n",
                "# config.input_gt.base_dir = \"../../output/mimic_cxr/manual_test_set/\" + source_csv_dir_name\n",
                "# # Target output dir (conll)\n",
                "# config.temp_gt.base_dir = \"../../output/mimic_cxr/coref/individual_conll_ground_truth/\" + source_csv_dir_name\n",
                "# config.temp_gt.force_run = False # Force to delete and recreate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 250/250 [00:00<00:00, 5679.43it/s]\n",
                        "100%|██████████| 250/250 [00:09<00:00, 26.60it/s]\n",
                        "100%|██████████| 250/250 [00:00<00:00, 8557.78it/s]\n",
                        "100%|██████████| 250/250 [00:08<00:00, 28.38it/s]\n"
                    ]
                }
            ],
            "source": [
                "check_and_remove_dirs(config.temp_gt.base_dir, config.temp_gt.force_run)\n",
                "if os.path.exists(config.temp_gt.base_dir):\n",
                "    print(\"Individual conll files found and will be reused.\")\n",
                "else:\n",
                "    log_out = mimic_cxr_csv2conll.prepare_conll(config, config.input_gt, config.temp_gt, keep_0_coref_docs)\n",
                "    with open(config.output.run_statistic, \"a\", encoding=\"UTF-8\") as f:\n",
                "        f.write(f\"Source: {config.temp_gt.base_dir} \\n\")\n",
                "        f.write(json.dumps(log_out, indent=2))\n",
                "        f.write(\"\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Generate aggregrated conll files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# config.data_split.test_gt.target_doc_dir = \"../../output/mimic_cxr/manual_test_set/round1x2_new\"\n",
                "# config.data_split.test_gt.source_dir = \"../../output/mimic_cxr/coref/individual_conll_ground_truth/round1x2_new\"\n",
                "\n",
                "config.data_split.activate = [\"train_manual\", \"test_gt\"]\n",
                "\n",
                "# The docs in `target_doc_dir` dir is the docs we want to get from the `source_dir`\n",
                "config.data_split.train_manual.target_doc_dir = config.input_gt.base_dir # csv file path\n",
                "config.data_split.train_manual.source_dir = config.temp_gt.base_dir # Individual conll file path\n",
                "\n",
                "# config.data_split.test_gt.target_doc_dir = \"../../output/mimic_cxr/manual_test_set/round1x2\" \n",
                "# config.data_split.test_gt.source_dir = \"../../output/mimic_cxr/coref/individual_conll_ground_truth/round1x2\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "log_out = mimic_cxr_csv2conll.aggregrate_conll(config, keep_0_coref_docs)\n",
                "with open(config.output.log_file, \"a\", encoding=\"UTF-8\") as f:\n",
                "    for split_mode, details in log_out.items():\n",
                "        f.write(json.dumps({\n",
                "            \"output_folder\": split_mode,\n",
                "            \"details\": details\n",
                "        }, indent=2))\n",
                "        f.write(\"\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Generage jsonlines files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "config.longformer.source = [{'train': 'train_manual'}, {'dev': 'train_manual'}, {'test': 'test_gt'}]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model: longformer, Segment length: 4096\n"
                    ]
                }
            ],
            "source": [
                "log_msg = mimic_cxr_conll2jsonlines.invoke(config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['Wrote 400 documents to /home/yuxiangliao/PhD/workspace/VSCode_workspace/sr_coref/output/mimic_cxr/coref/longformer/train.4096.jsonlines',\n",
                            " 'Wrote 100 documents to /home/yuxiangliao/PhD/workspace/VSCode_workspace/sr_coref/output/mimic_cxr/coref/longformer/dev.4096.jsonlines',\n",
                            " 'Wrote 200 documents to /home/yuxiangliao/PhD/workspace/VSCode_workspace/sr_coref/output/mimic_cxr/coref/longformer/test.4096.jsonlines']"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "log_msg"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Output to:  /home/yuxiangliao/PhD/workspace/VSCode_workspace/sr_coref/output/mimic_cxr/coref/gold_keep0coref_new_all/round4_500_1234r3_new\n"
                    ]
                }
            ],
            "source": [
                "import shutil\n",
                "\n",
                "output_dir_conll = os.path.join(config.output.base_dir, \"conll\")\n",
                "dst = os.path.join(config.output.base_dir, base_output_dir_name, source_csv_dir_name, \"conll\")\n",
                "shutil.move(output_dir_conll, dst)\n",
                "\n",
                "output_dir_longformer = os.path.join(config.output.base_dir, \"longformer\")\n",
                "dst = os.path.join(config.output.base_dir, base_output_dir_name, source_csv_dir_name, \"longformer\")\n",
                "shutil.move(output_dir_longformer, dst)\n",
                "\n",
                "print(\"Output to: \", os.path.join(config.output.base_dir, base_output_dir_name, source_csv_dir_name))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Align keep0coref data to no0coref data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append(\"../../src\")\n",
                "sys.path.append(\"../../../../git_clone_repos/fast-coref/src/\")\n",
                "\n",
                "import shutil, os\n",
                "from data_preprocessing import mimic_cxr_csv2conll\n",
                "\n",
                "# 读取 no0coref的 train dev的doc id list，分别记录\n",
                "remove0coref_base_dir = \"../../output/mimic_cxr/coref/gold_no0coref_all\"\n",
                "remove0coref_dirs = [\n",
                "    # os.path.join(remove0coref_base_dir,\"mimic_manual_100\"),\n",
                "    # os.path.join(remove0coref_base_dir,\"mimic_manual_200\"),\n",
                "    # os.path.join(remove0coref_base_dir,\"mimic_manual_300\"),\n",
                "    # os.path.join(remove0coref_base_dir,\"mimic_manual_400\"),\n",
                "    os.path.join(remove0coref_base_dir,\"mimic_manual_500\")\n",
                "]\n",
                "\n",
                "# 读取 keep0coref的 train dev的doc id list，合并记录\n",
                "keep0coref_base_dir = \"../../output/mimic_cxr/coref/gold_keep0coref_new_all\"\n",
                "keep0coref_dirs = [\n",
                "    # os.path.join(keep0coref_base_dir,\"round4_100_1\"),\n",
                "    # os.path.join(keep0coref_base_dir,\"round4_200_1x2\"),\n",
                "    # os.path.join(keep0coref_base_dir,\"round4_300_123\"),\n",
                "    # os.path.join(keep0coref_base_dir,\"round4_400_1234\"),\n",
                "    os.path.join(keep0coref_base_dir,\"round4_500_1234r3_new\"),\n",
                "]\n",
                "\n",
                "individual_conll_gt_base_dir = \"../../output/mimic_cxr/coref/individual_conll_ground_truth\"\n",
                "individual_conll_gt_dirs = [\n",
                "    # os.path.join(individual_conll_gt_base_dir,\"round4_100_1\"),\n",
                "    # os.path.join(individual_conll_gt_base_dir,\"round4_200_1x2\"),\n",
                "    # os.path.join(individual_conll_gt_base_dir,\"round4_300_123\"),\n",
                "    # os.path.join(individual_conll_gt_base_dir,\"round4_400_1234\"),\n",
                "    os.path.join(individual_conll_gt_base_dir,\"round4_500_1234r3\"),\n",
                "]\n",
                "\n",
                "# 复制 no0coref的数据为keep0coref，然后把多余的0 coref data，添加到末尾\n",
                "# mimic_cxr_csv2conll.copy_and_paste_conll(input_conll_file, output_conll_file)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import random\n",
                "from data_preprocessing import mimic_cxr_csv2conll\n",
                "\n",
                "aligned_output_base_dir = \"../../output/mimic_cxr/coref/gold_keep0coref_all_aligned\"\n",
                "\n",
                "for remove0coref_dir, keep0coref_dir, individual_conll_gt_dir in list(zip(remove0coref_dirs,keep0coref_dirs,individual_conll_gt_dirs)):\n",
                "    base_dir_name = os.path.basename(remove0coref_dir)\n",
                "    aligned_output_dir = os.path.join(aligned_output_base_dir, base_dir_name)\n",
                "    \n",
                "    no_0_coref_doc_list_dict = {\"train\":[],\"dev\":[]}\n",
                "    has_0_coref_doc_list_dict = {\"train\":[],\"dev\":[]}\n",
                "    with open(os.path.join(remove0coref_dir,\"longformer\", \"train.4096.jsonlines\"), \"r\") as f:\n",
                "        no_0_coref_doc_list_dict[\"train\"] = f.readlines()\n",
                "    with open(os.path.join(remove0coref_dir,\"longformer\", \"dev.4096.jsonlines\"), \"r\") as f:\n",
                "        no_0_coref_doc_list_dict[\"dev\"] = f.readlines()\n",
                "    \n",
                "    with open(os.path.join(keep0coref_dir,\"longformer\", \"train.4096.jsonlines\"), \"r\") as f:     \n",
                "        has_0_coref_doc_list_dict[\"train\"] = f.readlines()\n",
                "    with open(os.path.join(keep0coref_dir,\"longformer\", \"dev.4096.jsonlines\"), \"r\") as f:     \n",
                "        has_0_coref_doc_list_dict[\"dev\"] = f.readlines()\n",
                "    \n",
                "    # find docs that are not included in the no0coref dataset (i.e. find the 0 coref docs)\n",
                "    unused_doc_list = [*has_0_coref_doc_list_dict[\"train\"],*has_0_coref_doc_list_dict[\"dev\"]]\n",
                "    doc_indexing_list = list(map(lambda x: json.loads(x)[\"doc_key\"], unused_doc_list))\n",
                "    unused_doc_id_list = doc_indexing_list.copy()\n",
                "    for split, no_0coref_list in no_0_coref_doc_list_dict.items():\n",
                "        for line in no_0coref_list:\n",
                "            doc_id = json.loads(line)[\"doc_key\"]\n",
                "            if doc_id in unused_doc_id_list:\n",
                "                unused_doc_id_list.remove(doc_id)\n",
                "\n",
                "    for split, no_0coref_list in no_0_coref_doc_list_dict.items():\n",
                "        expected_len = len(has_0_coref_doc_list_dict[split])\n",
                "        current_len = len(no_0_coref_doc_list_dict[split])\n",
                "        needed_doc_num = expected_len - current_len\n",
                "        # Generate /longformer/xxx.jsonlines file\n",
                "        output_lines = no_0coref_list.copy()\n",
                "        \n",
                "        while needed_doc_num:\n",
                "            doc_id = unused_doc_id_list.pop(0)\n",
                "            doc_line = unused_doc_list[doc_indexing_list.index(doc_id)]\n",
                "            idx = random.randint(0, len(output_lines))\n",
                "            output_lines.insert(idx, doc_line)\n",
                "            needed_doc_num -= 1\n",
                "        \n",
                "        # Generate /longformer/split.jsonlines files\n",
                "        check_and_create_dirs(os.path.join(aligned_output_dir,\"longformer\"))\n",
                "        with open(os.path.join(aligned_output_dir,\"longformer\",split+\".4096.jsonlines\"),\"w\") as f:\n",
                "            f.write(\"\\n\".join([i.strip() for i in output_lines]))\n",
                "            \n",
                "        # Generate /conll/split.conll files\n",
                "        check_and_create_dirs(os.path.join(aligned_output_dir,\"conll\"))\n",
                "        for line in output_lines:\n",
                "            doc_id, section_name, _ = json.loads(line)[\"doc_key\"].split(\"_\")\n",
                "            input_conll_file = os.path.join(individual_conll_gt_dir,section_name,doc_id+\".conll\")\n",
                "            output_conll_file = os.path.join(aligned_output_dir,\"conll\",split+\".conll\")\n",
                "            mimic_cxr_csv2conll.copy_and_paste_conll(input_conll_file, output_conll_file)\n",
                "            \n",
                "    \n",
                "    # copy the test jsonlines from keep0coref dataset\n",
                "    shutil.copy(os.path.join(keep0coref_dir,\"longformer\",\"test.4096.jsonlines\"),\n",
                "                os.path.join(aligned_output_dir,\"longformer\",\"test.4096.jsonlines\"))\n",
                "    \n",
                "    # copy the test conll from keep0coref dataset\n",
                "    shutil.copy(os.path.join(keep0coref_dir,\"conll\",\"test.conll\"),\n",
                "                os.path.join(aligned_output_dir,\"conll\",\"test.conll\"))\n",
                "    "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Flexiable train and dev dataset\n",
                "\n",
                "For round4_500_1234r3_unsplit, we aggregrate the train and dev data into the same .jsonlines file.\n",
                "So, we can experiment with different number of train/dev data for model trainig.\n",
                "\n",
                "The .jsonlines file should be name as `train_dev.4096.jsonlines`\n",
                "\n",
                "Notice that the conll files do not change."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "source_input_dir = os.path.abspath(\"../../output/mimic_cxr/coref/gold_keep0coref_new_all_aligned/round4_500_1234r3_new\")\n",
                "output_dir = os.path.abspath(\"../../output/mimic_cxr/coref/gold_keep0coref_new_all_aligned/round4_500_1234r3_new_unsplit\")\n",
                "\n",
                "import sys\n",
                "sys.path.append(\"../../src\")\n",
                "\n",
                "import shutil\n",
                "from common_utils.common_utils import check_and_remove_dirs\n",
                "\n",
                "check_and_remove_dirs(output_dir, True)\n",
                "# copy files\n",
                "shutil.copytree(source_input_dir, output_dir)\n",
                "# rename the train file to train_dev\n",
                "os.rename(os.path.join(output_dir,\"longformer\",\"train.4096.jsonlines\"), \n",
                "          os.path.join(output_dir,\"longformer\",\"train_dev.4096.jsonlines\"))\n",
                "\n",
                "# copy the dev file to the new train_dev file\n",
                "with open(os.path.join(output_dir,\"longformer\",\"dev.4096.jsonlines\"),\"r\",encoding=\"utf-8\") as f_in:\n",
                "    dev_lines = f_in.readlines()\n",
                "with open(os.path.join(output_dir,\"longformer\",\"train_dev.4096.jsonlines\"),\"a\",encoding=\"utf-8\") as f_out:\n",
                "    f_out.write(\"\\n\")\n",
                "    f_out.writelines(dev_lines)\n",
                "    \n",
                "os.remove(os.path.join(output_dir,\"longformer\",\"dev.4096.jsonlines\"))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9.12 ('corenlp')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.16"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "bb6968a69f778f9e728e35b65cd79a0dbef5b20465434381676f63f710dc4a24"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
