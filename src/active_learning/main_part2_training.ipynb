{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active learning - part 2 - resolve annotation and model training\n",
    "\n",
    "1. communicate_brat_server：从BRATR服务器下载数据\n",
    "2. process_brat_annotation：解析注释数据\n",
    "3. build_training_data：生成训练数据\n",
    "4. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src/\")\n",
    "sys.path.append(\"../../../../git_clone_repos/fast-coref/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "config = None\n",
    "with initialize(version_base=None, config_path=\"../config\", job_name=\"active_learning\"):\n",
    "    config = compose(config_name=\"active_learning\")\n",
    "# print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download annotated data from the BRAT server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.21s/it]\n",
      "100%|██████████| 10/10 [00:12<00:00,  1.21s/it]\n",
      "3it [00:29,  9.70s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from active_learning.communicate_brat_server import RemoteConnection\n",
    "\n",
    "brat_server_basedir_name = f\"iter_{config.current_iter}\"\n",
    "brat_server_dir = os.path.join(config.remote_server.brat.data_dir, brat_server_basedir_name)\n",
    "\n",
    "brat_finished_parent_dir = os.path.split(config.output.brat.finished_dir)[0]\n",
    "brat_finished_base_dir_name = os.path.split(config.output.brat.finished_dir)[1]\n",
    "\n",
    "hostname = config.remote_server.brat.hostname\n",
    "username = config.remote_server.brat.username\n",
    "password = config.remote_server.brat.password\n",
    "connection = RemoteConnection(hostname, username, password)\n",
    "connection.get_all(\n",
    "    brat_server_dir,\n",
    "    brat_finished_parent_dir,\n",
    ")\n",
    "connection.close_client()\n",
    "\n",
    "# Rename local dir\n",
    "os.rename(\n",
    "    os.path.join(brat_finished_parent_dir, brat_server_basedir_name),\n",
    "    os.path.join(brat_finished_parent_dir, brat_finished_base_dir_name),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Resolve brat data and build training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data is created at: \n",
      " /home/yuxiangliao/PhD/workspace/VSCode_workspace/str_rep_coref/output/mimic_cxr/active_learning/iter_1/model_training_data\n"
     ]
    }
   ],
   "source": [
    "from active_learning.process_brat_annotation import resolve_brat\n",
    "from active_learning.build_training_data import (\n",
    "    build_aggregrated_conll,\n",
    "    concat_previous_conll_and_jsonlines,\n",
    "    build_individua_conll,\n",
    "    build_jsonlines,\n",
    ")\n",
    "\n",
    "resolve_brat(config)\n",
    "build_individua_conll(config)\n",
    "build_aggregrated_conll(config)\n",
    "build_jsonlines(config)\n",
    "concat_previous_conll_and_jsonlines(config)\n",
    "\n",
    "print(\"The training data is created at: \\n\", config.output.model_training_data.base_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Move training data to the model's resource dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yuxiangliao/PhD/workspace/git_clone_repos/fast-coref/coref_resources/data/mimic_active_learning_iter_1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "source_dir = config.output.model_training_data.base_dir\n",
    "des_dir = config.coref_model.dataset_dir\n",
    "# copy the subdirs from source_dir to des_dir\n",
    "shutil.copytree(source_dir, des_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create model's config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from active_learning.utils import get_trainset_size\n",
    "\n",
    "dataset_name = config.coref_model.dataset_name\n",
    "num_train_docs = get_trainset_size(config)\n",
    "\n",
    "content = f\"\"\"{dataset_name}:\n",
    "\\tname: \"{dataset_name}\"\n",
    "\\tcluster_threshold: 2  # Singletons are ignored for evaluation (also not annotated)\n",
    "\\tcanonical_cluster_threshold: 2\n",
    "\\ttargeted_eval: False\n",
    "\\tnum_train_docs: {num_train_docs}\n",
    "\\tnum_dev_docs: 25\n",
    "\\tnum_test_docs: 200\n",
    "\\thas_conll: True\n",
    "\\tsingleton_file: null\n",
    "\"\"\"\n",
    "\n",
    "dataset_conf_file = os.path.join(config.coref_model.conf_base_dir,\"datasets\",f\"{dataset_name}.yaml\")\n",
    "with open(dataset_conf_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = f\"\"\"# @package _global_\n",
    "\n",
    "defaults:\n",
    "\\t- override /datasets: {dataset_name}\n",
    "\\t- override /trainer: train.yaml\n",
    "\\t- override /model/doc_encoder/transformer: longformer_joint_arcca_local\n",
    "\n",
    "trainer:\n",
    "\\tlog_frequency: 50\n",
    "\\tpatience: 10\n",
    "\\tmax_evals: 100\n",
    "\\teval_per_k_steps: {num_train_docs}\n",
    "\n",
    "model:\n",
    "\\tdoc_encoder:\n",
    "\\t\\tadd_speaker_tokens: True\n",
    "\\t\\tfinetune: False\n",
    "\"\"\"\n",
    "\n",
    "dataset_conf_file = os.path.join(config.coref_model.conf_base_dir,\"experiment\",\"arcca_exp_10.yaml\")\n",
    "with open(dataset_conf_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sr_coref",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
